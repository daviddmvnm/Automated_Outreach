{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gender_guesser.detector as gender\n",
    "from human_mimic import human_sleep, human_scroll, random_hover\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LOGIN**\n",
    "- attempted changes, one time manual login, then automatic saving of cookies until you're kicked off, persistnet long lasting session as if you were a real user\n",
    "- better User Experience then manually scraping a cookie\n",
    "- I'm also going to reduce the number of top level functions because I learnt what a class really is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Manual login required.\n",
      "Please login in the Chrome window. Waiting...\n",
      "[✓] Cookies saved to proto_cookies.pkl\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pickle, os, time, random\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "class SessionManager:\n",
    "    def __init__(self, cookie_path=\"your_cookies.pkl\", use_user_profile=False, user_profile_path=None):\n",
    "        self.cookie_path = cookie_path\n",
    "        self.use_user_profile = use_user_profile\n",
    "        self.user_profile_path = user_profile_path\n",
    "        self.driver = self._init_driver()\n",
    "\n",
    "    def _init_driver(self):\n",
    "        chromedriver_autoinstaller.install()\n",
    "        options = Options()\n",
    "        options.add_argument(\"--user-agent=Mozilla/5.0\")\n",
    "\n",
    "        if self.use_user_profile:\n",
    "            if self.user_profile_path:\n",
    "                options.add_argument(f\"--user-data-dir={self.user_profile_path}\")\n",
    "                print(f\"[i] Using custom user profile: {self.user_profile_path}\")\n",
    "            else:\n",
    "                print(\"[i] User profile flag is on but no path specified.\")\n",
    "\n",
    "        return webdriver.Chrome(options=options)\n",
    "\n",
    "    \n",
    "    def _human_mimic(self):\n",
    "        human_scroll(self.driver, total_scrolls=5)\n",
    "        random_hover(self.driver)\n",
    "        human_sleep(2, 0.5)\n",
    "\n",
    "\n",
    "    def _save_cookies(self):\n",
    "        with open(self.cookie_path, \"wb\") as f:\n",
    "            pickle.dump(self.driver.get_cookies(), f)\n",
    "        print(f\"[SUCCESS] Cookies saved to {self.cookie_path}\")\n",
    "\n",
    "    def _load_cookies(self):\n",
    "        with open(self.cookie_path, \"rb\") as f:\n",
    "            cookies = pickle.load(f)\n",
    "            for cookie in cookies:\n",
    "                try:\n",
    "                    self.driver.add_cookie(cookie)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    def login(self):\n",
    "        self.driver.get(\"https://www.linkedin.com\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        if os.path.exists(self.cookie_path):\n",
    "            self._load_cookies()\n",
    "            self.driver.get(\"https://www.linkedin.com/feed/\")\n",
    "            time.sleep(3)\n",
    "\n",
    "            if \"feed\" in self.driver.current_url:\n",
    "                print(\"[SUCCESS] Logged in using saved cookies.\")\n",
    "                self._human_behavior()\n",
    "                return self.driver\n",
    "            else:\n",
    "                print(\"[ERROR] Saved cookies invalid or expired.\")\n",
    "        \n",
    "        print(\"Manual login required.\")\n",
    "        print(\"Please login in the Chrome window. Waiting...\")\n",
    "\n",
    "        self.driver.get(\"https://www.linkedin.com/login\")\n",
    "        input(\"Press [ENTER] after you've logged in...\")\n",
    "\n",
    "        if \"feed\" in self.driver.current_url:\n",
    "            self._save_cookies()\n",
    "            self._human_behavior()\n",
    "        else:\n",
    "            print(\"[ERROR] Login failed. Please try again.\")\n",
    "\n",
    "        return self.driver\n",
    "\n",
    "\n",
    "session = SessionManager()\n",
    "driver = session.login()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Navigate to PYMK tab**\n",
    "- change now takes a general input, e.g university of leeds, bristol, people work in consulting, etc.\n",
    "- longer wait times for lazy load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from human_mimic import human_sleep, random_hover\n",
    "import random\n",
    "\n",
    "def wait_and_open_target_tab(driver, target_label, max_retries=5, scroll_loops=8):\n",
    "    \"\"\"\n",
    "    Navigates to the 'My Network' page and tries to open a specific 'People You May Know' tab.\n",
    "\n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        target_label (str): The dynamic part of the tab label (e.g. 'University of Exeter')\n",
    "        max_retries (int): Max attempts before giving up\n",
    "        scroll_loops (int): How many scrolls to perform to trigger lazy loading\n",
    "    \"\"\"\n",
    "    print(f\"Trying to open 'People you may know from {target_label}' tab\")\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        print(f\"[{attempt + 1}/{max_retries}] Searching...\")\n",
    "\n",
    "        driver.get(\"https://www.linkedin.com/mynetwork/\")\n",
    "        human_sleep(5, 2)\n",
    "        random_hover(driver, \"a\")\n",
    "\n",
    "        for scroll in range(scroll_loops):\n",
    "            driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight);\")\n",
    "            print(f\"    Scrolled down {scroll + 1}/{scroll_loops}\")\n",
    "            human_sleep(2, 1.5)  # increased wait time per scroll\n",
    "\n",
    "            if scroll in {2, 5} and random.random() < 0.4:\n",
    "                print(\"   ...pausing to simulate reading\")\n",
    "                human_sleep(3, 1)\n",
    "\n",
    "        try:\n",
    "            show_all_btn = driver.find_element(\n",
    "                By.XPATH,\n",
    "                f\"//button[@aria-label='Show all suggestions for People you may know from {target_label}']\"\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({ behavior: 'smooth' });\", show_all_btn)\n",
    "            human_sleep(2, 1)\n",
    "\n",
    "            ActionChains(driver).move_to_element(show_all_btn).perform()\n",
    "            human_sleep(1.5, 0.5)\n",
    "\n",
    "            driver.execute_script(\"arguments[0].click();\", show_all_btn)\n",
    "            print(f\"Opened suggestions for: {target_label}\")\n",
    "            return True\n",
    "\n",
    "        except Exception:\n",
    "            print(\"Did not find the tab this time.\")\n",
    "\n",
    "            if random.random() < 0.4:\n",
    "                driver.get(\"https://www.linkedin.com/feed/\")\n",
    "                print(\"Went back to feed to reset.\")\n",
    "            else:\n",
    "                print(\"Retrying directly from My Network...\")\n",
    "\n",
    "            human_sleep(5, 2)\n",
    "\n",
    "    print(\"Gave up after too many attempts, are you sure you entered in the section name EXACTLY AS IT'S WRITTEN?.\")\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why I Chose a Two-Phase Scraping Pipeline (Option A)**\n",
    "\n",
    "We separate scraping into two distinct phases:\n",
    "\n",
    "1. **Phase 1 – Shallow Scrape (PYMK tab):**\n",
    "   - Collect basic metadata (name, role, profile URL) from the \"People You May Know\" view.\n",
    "   - Avoids interrupting LinkedIn’s scroll-based loading behavior.\n",
    "\n",
    "2. **Phase 2 – Deep Scrape (Full Profiles):**\n",
    "   - Later, visit each profile URL individually and save raw HTML for offline parsing.\n",
    "\n",
    "#### **Why This Is Better Than In-Loop HTML Scraping (Option B):**\n",
    "- Keeps the PYMK tab stable — avoids breaking the scroll loop.\n",
    "- Reduces suspicious behavior (less jumping between pages).\n",
    "- Faster and more efficient for gathering large batches.\n",
    "- Allows offline parsing and reprocessing without re-scraping.\n",
    "\n",
    "This design increases reliability and stealth while keeping the scraping modular and maintainable.\n",
    "\n",
    "\n",
    "# **Shallow Scraping**\n",
    "-No global seen_urls/people — it's self-contained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def scroll_and_extract_profiles(\n",
    "    driver,\n",
    "    pause_range=(2.5, 4.0),\n",
    "    streak_limit=5,\n",
    "    scrolls_per_loop=10,\n",
    "    scroll_factor=1.8,\n",
    "    max_profiles=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Scrolls through the suggestion tab and extracts new profile previews.\n",
    "\n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        pause_range: Tuple of (min, max) pause time between loops.\n",
    "        streak_limit: How many times in a row we can scroll with 0 new profiles.\n",
    "        scrolls_per_loop: Number of scrolls per iteration.\n",
    "        scroll_factor: Amount to scroll per step (adjusted randomly).\n",
    "        max_profiles: Optional int to stop scraping after reaching N profiles.\n",
    "    \"\"\"\n",
    "\n",
    "    seen_urls = set()\n",
    "    people = []\n",
    "    streak = 0\n",
    "    loop = 0\n",
    "\n",
    "    print(\"\\nStarting deep scroll & extract...\\n\")\n",
    "\n",
    "    def get_scroll_container(timeout=10):\n",
    "        try:\n",
    "            container = WebDriverWait(driver, timeout).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CSS_SELECTOR, \"#root > dialog > div > div:nth-child(2)\")\n",
    "                )\n",
    "            )\n",
    "            return container\n",
    "        except Exception:\n",
    "            print(\"   Scroll container not found.\")\n",
    "            return None\n",
    "\n",
    "    def extract_new_people():\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        cards = soup.find_all(\"a\", href=True)\n",
    "        new_count = 0\n",
    "\n",
    "        for card in cards:\n",
    "            href = card['href']\n",
    "            if not href.startswith(\"https://www.linkedin.com/in/\"):\n",
    "                continue\n",
    "            if href in seen_urls:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                paragraphs = card.find_all(\"p\")\n",
    "                if len(paragraphs) < 2:\n",
    "                    continue\n",
    "\n",
    "                name = paragraphs[0].get_text(strip=True)\n",
    "                headline = paragraphs[1].get_text(strip=True)\n",
    "\n",
    "                people.append({\n",
    "                    \"name\": name,\n",
    "                    \"headline\": headline,\n",
    "                    \"profile_url\": href\n",
    "                })\n",
    "                seen_urls.add(href)\n",
    "                new_count += 1\n",
    "\n",
    "                if max_profiles is not None and len(seen_urls) >= max_profiles:\n",
    "                    break  # stop collecting more this round\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"   Error parsing card:\", e)\n",
    "                continue\n",
    "\n",
    "        return new_count\n",
    "\n",
    "    while True:\n",
    "        loop += 1\n",
    "        print(f\"Loop {loop}\")\n",
    "\n",
    "        new_profiles = extract_new_people()\n",
    "        print(f\"   New: {new_profiles} | Total collected: {len(seen_urls)}\")\n",
    "\n",
    "        if max_profiles is not None and len(seen_urls) >= max_profiles:\n",
    "            print(f\"Reached max scrape limit ({max_profiles}). Stopping.\")\n",
    "            break\n",
    "\n",
    "        if new_profiles == 0:\n",
    "            streak += 1\n",
    "            print(f\"   No new profiles ({streak}/{streak_limit})\")\n",
    "            if streak >= streak_limit:\n",
    "                print(\"Too many empty scrolls. Ending loop.\")\n",
    "                break\n",
    "        else:\n",
    "            streak = 0\n",
    "\n",
    "        scroll_container = get_scroll_container()\n",
    "        if scroll_container and scroll_container.size['height'] > 0:\n",
    "            try:\n",
    "                ActionChains(driver).move_to_element(scroll_container).perform()\n",
    "                human_sleep(0.5, 0.2)\n",
    "\n",
    "                for s in range(scrolls_per_loop):\n",
    "                    scroll_step = scroll_factor + random.uniform(-0.3, 0.3)\n",
    "                    driver.execute_script(\"\"\"\n",
    "                        let container = arguments[0];\n",
    "                        container.scrollTop += container.clientHeight * arguments[1];\n",
    "                    \"\"\", scroll_container, scroll_step)\n",
    "\n",
    "                    print(f\"      Scroll {s+1}/{scrolls_per_loop} [factor: {scroll_step:.2f}]\")\n",
    "                    human_sleep(random.uniform(1.5, 2.5))\n",
    "\n",
    "                try:\n",
    "                    see_more_button = driver.find_element(\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"#root > dialog > div > div > div > div > section > div > div > div > div._1xoe5hdi.cnuthtrs > button\"\n",
    "                    )\n",
    "                    if see_more_button:\n",
    "                        print(\"   Found 'See more' button — clicking.\")\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView({ behavior: 'smooth' });\", see_more_button)\n",
    "                        human_sleep(2, 1)\n",
    "                        ActionChains(driver).move_to_element(see_more_button).perform()\n",
    "                        human_sleep(1.5, 0.5)\n",
    "                        driver.execute_script(\"arguments[0].click();\", see_more_button)\n",
    "                        human_sleep(2, 1)\n",
    "                except Exception:\n",
    "                    print(\"   No 'See more' button found this round.\")\n",
    "\n",
    "                if loop % 3 == 0 and random.random() < 0.5:\n",
    "                    pause = random.uniform(5, 9)\n",
    "                    print(f\"   Taking a longer pause: {pause:.1f}s\")\n",
    "                    human_sleep(pause / 2, pause / 2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"   Failed to scroll:\", e)\n",
    "\n",
    "        else:\n",
    "            print(\"   Scroll container missing or collapsed.\")\n",
    "            streak += 1\n",
    "\n",
    "        human_sleep(*pause_range)\n",
    "\n",
    "    df = pd.DataFrame(people)\n",
    "    print(f\"\\nDone. Collected {len(df)} unique profiles.\\n\")\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
